{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3cbdfda",
   "metadata": {},
   "source": [
    "# Homework: turn the FakeNews text column into tfidf vectors the easy way\n",
    "  \n",
    "Vocab:\n",
    "- corpus: set of all documents in a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d68b60",
   "metadata": {},
   "source": [
    "## `news.csv` Data Set\n",
    "\n",
    "4 columns: \n",
    "- article id\n",
    "- article title\n",
    "- article text\n",
    "- lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67d217ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98c686da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the dataset: (6335, 4) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6903</td>\n",
       "      <td>Tehran, USA</td>\n",
       "      <td>\\nI’m not an immigrant, but my grandparents ...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7341</td>\n",
       "      <td>Girl Horrified At What She Watches Boyfriend D...</td>\n",
       "      <td>Share This Baylee Luciani (left), Screenshot o...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>95</td>\n",
       "      <td>‘Britain’s Schindler’ Dies at 106</td>\n",
       "      <td>A Czech stockbroker who saved more than 650 Je...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4869</td>\n",
       "      <td>Fact check: Trump and Clinton at the 'commande...</td>\n",
       "      <td>Hillary Clinton and Donald Trump made some ina...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2909</td>\n",
       "      <td>Iran reportedly makes new push for uranium con...</td>\n",
       "      <td>Iranian negotiators reportedly have made a las...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "5        6903                                        Tehran, USA   \n",
       "6        7341  Girl Horrified At What She Watches Boyfriend D...   \n",
       "7          95                  ‘Britain’s Schindler’ Dies at 106   \n",
       "8        4869  Fact check: Trump and Clinton at the 'commande...   \n",
       "9        2909  Iran reportedly makes new push for uranium con...   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  \n",
       "5    \\nI’m not an immigrant, but my grandparents ...  FAKE  \n",
       "6  Share This Baylee Luciani (left), Screenshot o...  FAKE  \n",
       "7  A Czech stockbroker who saved more than 650 Je...  REAL  \n",
       "8  Hillary Clinton and Donald Trump made some ina...  REAL  \n",
       "9  Iranian negotiators reportedly have made a las...  REAL  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the data\n",
    "df=pd.read_csv('data/news.csv')\n",
    "\n",
    "#Get shape and head\n",
    "shape = df.shape\n",
    "print(f\"shape of the dataset: {shape} \\n\")\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f97347f",
   "metadata": {},
   "source": [
    "## Machine Learning Framework API Patterns: Transformers and Estimators\n",
    "\n",
    "Machine learning pipelines take a raw dataset and then transform the data over a series of steps and finally outputs some prediction. For example, text documents -> tokenization -> word counts -> idf calculation -> tfidf calculation -> build classification model -> predictions\n",
    "\n",
    "The individual steps in a pipeline are called Transformers. Some Transformers can take the input data and directly transform it without any understanding of the full dataset. For example, a stopword filter transformer might take a set of document token arrays, and remove stopwords from each document token array.\n",
    "\n",
    "Transformers usually have a `transform()` function to do it's work. \n",
    "\n",
    "Other types of Transformers need to capture some understanding of the entire dataset before it can Transform the input data. For example a min/max normalization transformer needs to pass over the entire data set once to figure out the minimum and maximum values, then it'll pass over the dataset a second time and calculate the min/max normalization. \n",
    "\n",
    "These types of Transformers are called Estimators and usually have a `fit()` function which will do the first pass over the data and calculate it's required state (min and max values), and returns a Transformer that has this state. Then you can call `transform()` on this object to transform the data (calc min/max norms)\n",
    "\n",
    "Scikit Learn, Spark, and a few others I've run across follow this pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2da764",
   "metadata": {},
   "source": [
    "## Word Tokenization & Token Counts\n",
    "\n",
    "CountVectorizer: Converts a collection of text documents into a matrix of token counts\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c86c6ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 2, 0, 1, 0, 1, 1, 0, 1],\n",
       "       [1, 0, 0, 1, 1, 0, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['this is the first document',\n",
    "           'this document is the second document',\n",
    "           'and this is the third one',\n",
    "           'is this the first document',         \n",
    "         ]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "estimator = vectorizer.fit(corpus)\n",
    "X_train_counts = estimator.transform(corpus)\n",
    "\n",
    "X_train_counts = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# dictionary of all terms in corpus\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "# token count marix from corpus\n",
    "X_train_counts.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d90ed2b",
   "metadata": {},
   "source": [
    "### Task: Run the text column from the FakeNews dataset through the CountVectorizer\n",
    "\n",
    "Task 1a:\n",
    "- turn the raw text column into count vectors\n",
    "  - Try to keep all data in the pandas dataframe. So when creating the count vectors, put them back into the pandas dataframe as a new column\n",
    "- what is the size of the corpus dictionary?\n",
    "- what is the shape of the fitted document token count matrix\n",
    "- from a TF-IDF perspective, how does the output of the CountVectorizer relate to the TF-IDF calculation\n",
    "\n",
    "Task 1b:\n",
    "- look at the CountVectorizer documentation and find the `min_df` and `max_df` parameters to the CountVectorizer constructor\n",
    "  - use these params to filter out terms (tokens) that are only in 5 documents or less\n",
    "  - use these params to filter out terms (tokens) that are in 95% of the docuemnts or more\n",
    "- what is the size of the corpus dictionary?\n",
    "- why are these two types of term filters useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f43e48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the size of the corpus dictionary?\n",
      "67659\n",
      "\n",
      "what is the shape of the fitted document token count matrix\n",
      "(6335, 67659)\n",
      "\n",
      "The output of the CounterVectorizer shows TF(Term Frequency)\n",
      "\n",
      "Terms that were ignoered\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# build count matrix for Fake News dataset\n",
    "corpus_fakenews = df[\"text\"]\n",
    "vectorizer_fakenews = CountVectorizer()\n",
    "estimator_fakenews = vectorizer_fakenews.fit_transform(corpus_fakenews)\n",
    "\n",
    "print(\"what is the size of the corpus dictionary?\")\n",
    "print(len(vectorizer_fakenews.get_feature_names()))\n",
    "\n",
    "print()\n",
    "print(\"what is the shape of the fitted document token count matrix\")\n",
    "print(estimator_fakenews.toarray().shape)\n",
    "\n",
    "# A mapping of terms to feature indices.\n",
    "# print(sorted(vectorizer_fakenews.vocabulary_.items()))\n",
    "# print(estimator_fakenews.toarray())\n",
    "print()\n",
    "print(\"The output of the CounterVectorizer shows TF(Term Frequency)\")\n",
    "\n",
    "print()\n",
    "print(\"Terms that were ignoered\")\n",
    "print(sorted(vectorizer_fakenews.stop_words_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64e71816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the size of the corpus dictionary?\n",
      "24021\n",
      "\n",
      "Terms that were ignoered\n",
      "\n",
      "why are these two types of term filters useful?\n",
      "- It is helpful on improving performance because those two parameters filter out non influential terms. Also it helps \n",
      "      improve quality of output for analytical purposes\n"
     ]
    }
   ],
   "source": [
    "vectorizer_fakenews_2 = CountVectorizer(max_df=0.95, min_df=5)\n",
    "estimator_fakenews_2 = vectorizer_fakenews_2.fit_transform(corpus_fakenews)\n",
    "\n",
    "print(\"what is the size of the corpus dictionary?\")\n",
    "print(len(vectorizer_fakenews_2.get_feature_names()))\n",
    "\n",
    "print()\n",
    "print(\"Terms that were ignoered\")\n",
    "# print(vectorizer_fakenews_2.stop_words_)\n",
    "\n",
    "print()\n",
    "print(\"why are these two types of term filters useful?\")\n",
    "print(\"\"\"- It is helpful on improving performance because those two parameters filter out non influential terms. Also it helps \n",
    "      improve quality of output for analytical purposes\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f352300",
   "metadata": {},
   "source": [
    "## Create TFIDF Vectors\n",
    "\n",
    "TfidfTransformer: Converts a count matrix into a tfidf matrix\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f604c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524],\n",
       "       [0.        , 0.6876236 , 0.        , 0.28108867, 0.        ,\n",
       "        0.53864762, 0.28108867, 0.        , 0.28108867],\n",
       "       [0.51184851, 0.        , 0.        , 0.26710379, 0.51184851,\n",
       "        0.        , 0.26710379, 0.51184851, 0.26710379],\n",
       "       [0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=True, smooth_idf=True).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "\n",
    "# dictionary term idf values\n",
    "print(tf_transformer.idf_)\n",
    "\n",
    "# token count marix from corpus\n",
    "X_train_tf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7ae2be",
   "metadata": {},
   "source": [
    "### Task: Run the token count data (that used the `min_df` and `max_df` parameters) from the FakeNews dataset through the TfidfTransformer\n",
    "\n",
    "- What's different between the counts matrix and the tfidf matrix?\n",
    "\n",
    "so much easier than manually calculating tfidf right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5afcb2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.67646549 2.64086082 7.96224346 ... 6.5759491  7.1149456  7.80809278]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.00924914, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.01247304, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate tfidf for the text column in the FakeNews dataset\n",
    "tf_transformer_fakenews = TfidfTransformer(use_idf=True, smooth_idf=True).fit(estimator_fakenews_2)\n",
    "X_train_tf_fakenews = tf_transformer_fakenews.transform(estimator_fakenews_2)\n",
    "\n",
    "# dictionary term idf values\n",
    "print(tf_transformer_fakenews.idf_)\n",
    "\n",
    "# token count marix from corpus\n",
    "X_train_tf_fakenews.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ea9f136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.00924914, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.01247304, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just try to calculate TFIDF with TfidfVectorizer module\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=5)\n",
    "tfidf_vectorizer.fit(df[\"text\"])\n",
    "tfidf_vectorizer.transform(df[\"text\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c78e7778-0206-4675-b525-bcb61b0fe4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorizer.get_feature_names()\n",
    "tfidf_vectorizer.get_stop_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973327e9",
   "metadata": {},
   "source": [
    "## Machine Learning Pipelines\n",
    "\n",
    "Most ML frameworks have a pipeline framework, where you can add multiple different transformers into a parent transformer, then you only all `fit()` and `transform()` on the pipeline object. Internally the pipeline will call `fit()` and `transform()` on each individual transformer and output the final matrix of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03eedea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524],\n",
       "       [0.        , 0.6876236 , 0.        , 0.28108867, 0.        ,\n",
       "        0.53864762, 0.28108867, 0.        , 0.28108867],\n",
       "       [0.51184851, 0.        , 0.        , 0.26710379, 0.51184851,\n",
       "        0.        , 0.26710379, 0.51184851, 0.26710379],\n",
       "       [0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('count', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "]).fit(corpus)\n",
    "\n",
    "X = pipe.transform(corpus)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "621bf609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count vectorizer dictionary: ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "\n",
      "tfidf transformer's idf data: [1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(f\"count vectorizer dictionary: {pipe['count'].get_feature_names()}\")\n",
    "print()\n",
    "print(f\"tfidf transformer's idf data: {pipe['tfidf'].idf_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a647ecf",
   "metadata": {},
   "source": [
    "### Task: Build a Pipeline to generate a tfidf document matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "88b39bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.00924914, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.01247304, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build and a ML pipeline to calc tfidf on the text column of the FakeNews dataset\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('count', CountVectorizer(max_df=0.95, min_df=5)),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True, smooth_idf=True)),\n",
    "]).fit(corpus_fakenews)\n",
    "\n",
    "X = pipe.transform(corpus_fakenews)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dd5626",
   "metadata": {},
   "source": [
    "## Extra Credit: Text Normalization\n",
    "\n",
    "There are different algorithms for text normalization, such as stemming and lemitization. These algorithms aren't build into scikit-learn, but other text processing libraries like `nltk` have implementations. Both have pros and cons. I really like lemitization, but it has a heavier processing cost. \n",
    "\n",
    "Pick one, stemming or lemitization, and integrate it into your raw text to tfidf pipeline (there are articles out there on how to integrate `nltk` into a scikit-learn pipeline).  \n",
    "\n",
    "How did this change the vocabulary size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a62d12d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the dataset: (6335, 4) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read the data\n",
    "df=pd.read_csv('data/news.csv')\n",
    "\n",
    "#Get shape and head\n",
    "shape = df.shape\n",
    "print(f\"shape of the dataset: {shape} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d452566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "raw_data = df[\"text\"]\n",
    "stemmer = nltk.SnowballStemmer('english')\n",
    "\n",
    "def nltkProcessing(doc):\n",
    "    words = nltk.word_tokenize(doc)\n",
    "    stemmed_words = []\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "\n",
    "        if word.isnumber:\n",
    "            if int(word) <= 100:\n",
    "                word = 100\n",
    "            elif int(word) <= 500:\n",
    "                word = 500\n",
    "            elif int(word) <= 1000:\n",
    "                word = 1000\n",
    "            elif int(word) <= 10000:\n",
    "                word = 10000\n",
    "            elif int(word) <= 100000:\n",
    "                word = 100000\n",
    "            else:\n",
    "                word = 100001\n",
    "\n",
    "        if word.isnumber:\n",
    "            word = \"_number_token_\"\n",
    "\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word = stemmer.stem(word)\n",
    "            stemmed_words.append(word)\n",
    "\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "787c454f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.03882442, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.07098608],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('count', CountVectorizer(max_df=0.95, min_df=5, tokenizer=nltkProcessing)),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True, smooth_idf=True)),\n",
    "]).fit(raw_data)\n",
    "\n",
    "X = pipe.transform(raw_data)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9f9e5f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count vectorizer dictionary: 14120\n",
      "\n",
      "67659 --> 24021 (+ max_df, min_df) --> 14120 (+ nltk)\n"
     ]
    }
   ],
   "source": [
    "print(f\"count vectorizer dictionary: {len(pipe['count'].get_feature_names())}\")\n",
    "print()\n",
    "print(\"67659 --> 24021 (+ max_df, min_df) --> 14120 (+ nltk)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81af774a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b94dd9aa-75d2-49ef-91d4-6e4ee04af89f",
   "metadata": {},
   "source": [
    "## Other Topics\n",
    "\n",
    "Feature Space Size to Training Data relationship\n",
    "- as feature space gets larger so does the complexity of the model\n",
    "  - the curse of dimensionality\n",
    "  - https://en.wikipedia.org/wiki/Curse_of_dimensionality \n",
    "  - https://towardsdatascience.com/the-curse-of-dimensionality-50dc6e49aa1e \n",
    "- uncorrelated features: N-1 features, where N is the sample size)\n",
    "- correlated features: square root of N\n",
    "- the more features, the more training data you need\n",
    "- ngrams can be very effective, but explode the feature space size, which requires a LOT more training data\n",
    "\n",
    "Feature Selection\n",
    "- minDF / maxDF are one type of feature selection\n",
    "- chi squared or mutial information is another \n",
    "\n",
    "Feature Reduction (or dimensionality reduction)\n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db6f510-89ad-4ed1-9f56-d0db5d5ab5c8",
   "metadata": {},
   "source": [
    "10,000 x 67,659\n",
    "5000 fake\n",
    "5000 real\n",
    "\n",
    "10,000 x 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975d0cdb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602cab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "5^5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb0ebba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
