{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I'm going to go through the same dataset in the `data_exploration` and `tfidf` notebooks, but use FastAI's NLP libraries to try to build a classifier for fake vs real with a neural net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import get_text_files, Path, TextDataLoaders, language_model_learner, text_classifier_learner, accuracy, Perplexity, AWD_LSTM, AWD_QRNN\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"/home/g-clef/turbo_stuff/fake_news_analysis/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to make a \"DataLoader\" in fastai terminology, which is an object that loads the data from disk and prepares it for feeding to the learning model in batches. The dataloader for text also handles a bunch of lexical parsing, stemming, punctuation handling, etc.\n",
    "\n",
    "This next step raises a numpy warning every time, and does in the fastai documentation as well, so I'm going to ignore that for the moment.\n",
    "\n",
    "In earlier uses of this, I was experimenting with lowering the batch size (`bs=4`) and block size `block_size=4`. That was mostly related to tryign to get a languate learner to work and fit in RAM. For the text classifier that mostly seemed to just slow things down and make determining a good learning rate difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g-clef/anaconda3/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "dls = TextDataLoaders.from_csv(path=path, csv_fname=\"news.csv\", text_col=[\"title\", \"text\"], label_col=\"label\", valid_pct=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxfld 1 xxmaj re : xxmaj why xxmaj are xxmaj so xxmaj many xxmaj people xxmaj choosing xxmaj to xxmaj leave xxmaj the xxmaj united xxmaj states xxmaj permanently ? xxfld 2 xxmaj why xxmaj are xxmaj so xxmaj many xxmaj people xxmaj choosing xxmaj to xxmaj leave xxmaj the xxmaj united xxmaj states xxmaj permanently ? xxmaj august 11th , 2013 \\n xxmaj have things gotten so bad that it is time to leave the xxmaj united xxmaj states for good ? xxmaj that is a question that a lot of xxmaj americans are dealing with these days , and an increasing number of them are choosing to leave the country of their birth permanently . xxmaj some are doing it for tax reasons , some are doing it because they believe the future is brighter elsewhere , and others are doing it because they are very distressed</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos xxfld 1 xxmaj fascism xxmaj in xxmaj india - xxunk xxfld 2 in xxmaj communal xxmaj harmony — by xxmaj xxunk xxmaj xxunk — xxmaj november 5 , 2016 \\n xxmaj on xxmaj monday the 31st of xxmaj october 2016 the xxmaj xxunk xxmaj pradesh police assassinated eight alleged associates of the xxmaj students ’ xxmaj islamic xxmaj movement of xxmaj india ( simi ) after an alleged escape from prison near xxmaj bhopal . xxmaj this was not coincidentally a day when the xxmaj prime xxmaj minister xxmaj narendra xxmaj modi was hyping xxmaj hindu nationalism on xxmaj diwali , the xxmaj hindu festival that celebrates the return of the xxunk god xxmaj ram to xxmaj xxunk in present day xxmaj xxunk xxmaj pradesh . xxmaj the assassination was most certainly a celebration of xxmaj diwali and xxmaj hindu nationalism by assassination of xxmaj muslims . xxmaj narendra</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos xxfld 1 xxup lucifer in the xxmaj temple of the xxmaj dog xxup ii xxfld 2 xxup isis xxmaj takes xxmaj out xxup xxunk xxmaj abrams xxmaj tank with xxmaj american xxup tow xxmaj missile ( video ) ‹ › xxmaj bio by xxmaj jack xxmaj heart : xxmaj my earliest memories were of being surrounded by machinery and a constant deep mechanical humming rose and fell like the breath of xxunk sleep . xxmaj maybe it was the \" mother ship \" or the \" montauk underground \" like xxmaj preston xxmaj nichols author of the xxmaj the xxmaj montauk xxmaj projects would later claim but i am inclined to believe it was the post natal care room at xxmaj xxunk xxmaj medical xxmaj center in xxmaj brooklyn where i was born to a well to do family . i grew up in xxmaj brooklyn . xxmaj my</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is a lot of pre-parsed text. There's a lot to talk about how this parsing is working. FastAI offers multiple ways to \"tokenize\" text. By default, a TextDataLoader will use a tokenizer called `spaCy`. FastAI mostly uses spacy to split on words in an intelligent way (don't split punctuation in the middle of a number or a price, for example, but do split `it's` into `it s`. FastAI wraps that in a class of its own, so if you look in the TextDataLoader, it will be called a \"WordTokenizer\", to allow them to swap that out if they find something better than spaCy. \n",
    "\n",
    "FastAI will go beyond spacy word splitting, to help identify special areas of the strings. FastAI will identify words and add extra tokens to indicate other information about the words it tokenizes. The tokens above that start with `xx` are special tokens added by FastAI. For example `xxbos` means \"beginning of string\" to mark the beginning of the set of things being tokenized, `xxmaj` means that the next word started with a capital letter, `xxunk` means the next word was unknown, etc. `xxup` means all letters were uppercase, `xxfld 1` means this data was in field 1. This is how FastAI handles the fact that data may have come from more than one field in the incoming CSV: it tags the text that's from that field with a special token at the beginning of that field's data. \n",
    "\n",
    "If spaces aren't useful,  you may ned to use a \"subword\" tokenizer. (note: the book claims that subword tokenizers can handle genomic sequences and MIDI notiation...possible to use subword tokenizers and language models with them to do file analysis? Files could be treated as just a string of hexadecimal values at that point.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on, next we make a text_classifier learner. It may be tempting to try to create a full language learner here, but in my experience the language learners required more RAM than I had available in my GPU (and I've got 8GB).\n",
    "\n",
    "The difference between a language learner and a text classifier appears to be that a language learner learns the corpus to allow it to make predictions about later words given a starting point. So it will generate or finish sentences in the style of the corpus given a starting point. A clasifier on the other hand, will just give you a classification of which label a given text belongs in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.3, metrics=[accuracy, Perplexity()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=0.03630780577659607, lr_steep=0.0010000000474974513)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a fairly typical learning rate curve from a FastAI perspective: It's semi-bathtub-shaped, with a long section with a steeply-dropping loss in the middle. That implies the techniques used by the vanilla FastAI learners will work fairly well here without much modification. Let's try just one cycle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.399221</td>\n",
       "      <td>0.297955</td>\n",
       "      <td>0.873717</td>\n",
       "      <td>1.347101</td>\n",
       "      <td>01:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy 87% on one round? Not bad at all. Let's try again, but train for more than one epoch. (this may take a *long* time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.497114</td>\n",
       "      <td>0.359929</td>\n",
       "      <td>0.856354</td>\n",
       "      <td>1.433228</td>\n",
       "      <td>01:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.395303</td>\n",
       "      <td>0.291344</td>\n",
       "      <td>0.872139</td>\n",
       "      <td>1.338225</td>\n",
       "      <td>01:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.345691</td>\n",
       "      <td>0.261555</td>\n",
       "      <td>0.889503</td>\n",
       "      <td>1.298948</td>\n",
       "      <td>01:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.315118</td>\n",
       "      <td>0.254234</td>\n",
       "      <td>0.885556</td>\n",
       "      <td>1.289473</td>\n",
       "      <td>01:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.281368</td>\n",
       "      <td>0.244851</td>\n",
       "      <td>0.888713</td>\n",
       "      <td>1.277431</td>\n",
       "      <td>01:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.258485</td>\n",
       "      <td>0.251115</td>\n",
       "      <td>0.888713</td>\n",
       "      <td>1.285458</td>\n",
       "      <td>01:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.245854</td>\n",
       "      <td>0.248446</td>\n",
       "      <td>0.889503</td>\n",
       "      <td>1.282032</td>\n",
       "      <td>01:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.241444</td>\n",
       "      <td>0.261919</td>\n",
       "      <td>0.887924</td>\n",
       "      <td>1.299422</td>\n",
       "      <td>01:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.3, metrics=[accuracy, Perplexity()])\n",
    "learn.fit_one_cycle(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "89% accuracy. Not much change from the original, and seems to have flatlined at that accuracy.\n",
    "\n",
    "One thing, though: the `drop_mult` is a bit of a magic number that I took from someone else's analysis. FastAI documentation says that `drop_mult` is a common weight to apply to all the kinds of dropout that the library can use. So, turning that up causes more dropout of all kinds, down causes less. Dropout is a method used to avoid overfitting data. With dropout, during training some of the activation values for the non-linear layers in the model are set to 0 for that round, to force the model to see whether simply removing that activation makes things better or worse. In the text classifier, there are actually multiple types of Dropout: at hidden, input, embed, and weight. These set the probability the learner will disable a given activation (in effect the % of activations that will be disabled in a  given batch). \n",
    "\n",
    "Let's see what happens if we change that value for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.491514</td>\n",
       "      <td>0.459929</td>\n",
       "      <td>0.779795</td>\n",
       "      <td>1.583961</td>\n",
       "      <td>01:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=1, metrics=[accuracy, Perplexity()])\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "well, that's a good deal less accurate on a first run. Perhaps a bit smaller?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.463297</td>\n",
       "      <td>0.375833</td>\n",
       "      <td>0.819258</td>\n",
       "      <td>1.456203</td>\n",
       "      <td>01:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.7, metrics=[accuracy, Perplexity()])\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "82%...still worse than the original 0.3 guess. Let's try setting it smaller than 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.401169</td>\n",
       "      <td>0.311660</td>\n",
       "      <td>0.874507</td>\n",
       "      <td>1.365691</td>\n",
       "      <td>01:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.2, metrics=[accuracy, Perplexity()])\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ooh. it's slightly better. Apparently we can't turn dropout off entirely (pytorch exceptions somewhere under the hood). Let's try setting it even lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.384740</td>\n",
       "      <td>0.294898</td>\n",
       "      <td>0.875296</td>\n",
       "      <td>1.342989</td>\n",
       "      <td>01:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.01, metrics=[accuracy, Perplexity()])\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "89% on one round. Not bad. Let's try multiple rounds with very little dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.428597</td>\n",
       "      <td>0.288850</td>\n",
       "      <td>0.880821</td>\n",
       "      <td>1.334892</td>\n",
       "      <td>01:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.313833</td>\n",
       "      <td>0.257216</td>\n",
       "      <td>0.894238</td>\n",
       "      <td>1.293325</td>\n",
       "      <td>01:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.257035</td>\n",
       "      <td>0.234647</td>\n",
       "      <td>0.906077</td>\n",
       "      <td>1.264462</td>\n",
       "      <td>01:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.203199</td>\n",
       "      <td>0.233756</td>\n",
       "      <td>0.902131</td>\n",
       "      <td>1.263336</td>\n",
       "      <td>01:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.01, metrics=[accuracy, Perplexity()])\n",
    "learn.fit_one_cycle(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit of an improvement in accuracy, but seems to be plateauing. One warning, though: the `train_loss` is getting smaller than the `valid_loss`, which seems to be plateau'ing. That's a sign that we need to turn the dropout back up, since it might be memorizing our input data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after reading a little more, it turns out that the fastai folks don't recommend doing multiple rounds like this. They say that better results are gained by doing a series of `fit_one_cycle(1)` calls, followed by `freeze_to(-x)` calls with increasing negative arguments.\n",
    "\n",
    "What that does is fit the last layers in the model one by one, rather than all at once. Their example does three layers, then unfreezes the whole model and does a fit across.\n",
    "\n",
    "Let's try that.\n",
    "\n",
    "One thing: this ran out of RAM at the `freeze_to(-3)` stage the first time I tried it, so I think we have to re-create the dataloader for this with a smaller batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g-clef/anaconda3/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "dls = TextDataLoaders.from_csv(path=path, csv_fname=\"news.csv\", text_col=[\"title\", \"text\"], label_col=\"label\", valid_pct=0.2, bs=8)\n",
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.2, metrics=[accuracy, Perplexity()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.422103</td>\n",
       "      <td>0.289651</td>\n",
       "      <td>0.883978</td>\n",
       "      <td>1.335962</td>\n",
       "      <td>03:38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.369540</td>\n",
       "      <td>0.214328</td>\n",
       "      <td>0.928177</td>\n",
       "      <td>1.239029</td>\n",
       "      <td>04:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.309182</td>\n",
       "      <td>0.171190</td>\n",
       "      <td>0.931334</td>\n",
       "      <td>1.186716</td>\n",
       "      <td>05:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.218524</td>\n",
       "      <td>0.099522</td>\n",
       "      <td>0.966851</td>\n",
       "      <td>1.104643</td>\n",
       "      <td>07:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.130463</td>\n",
       "      <td>0.165690</td>\n",
       "      <td>0.957380</td>\n",
       "      <td>1.180207</td>\n",
       "      <td>07:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.128321</td>\n",
       "      <td>0.328811</td>\n",
       "      <td>0.954223</td>\n",
       "      <td>1.389315</td>\n",
       "      <td>07:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.061000</td>\n",
       "      <td>0.192668</td>\n",
       "      <td>0.962115</td>\n",
       "      <td>1.212480</td>\n",
       "      <td>07:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.066970</td>\n",
       "      <td>0.154391</td>\n",
       "      <td>0.965272</td>\n",
       "      <td>1.166947</td>\n",
       "      <td>07:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "97%. Just wow. I'm a little concerned that this is over-fit, though, since the `train_loss` is smaller than the `valid_loss` (which is what the FastAI folks say to watch out for when training). Also, `valid_loss` is flatlining, and sometimes going *up*, which is another indication that it's overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go deeper into what it's doing, let's have a little fun & see what it does with predicting the classification of some sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('FAKE', tensor(0), tensor([1.0000e+00, 2.9312e-06]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starting_word = \"This is a lie\"\n",
    "learn.predict(starting_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('FAKE', tensor(0), tensor([9.9908e-01, 9.2202e-04]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starting = \"Is is possible that the length of a string has an impact on whether it's classified as fake? Let's see. This may also trigger on the fact that there are multiple sentences here.\"\n",
    "learn.predict(starting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('FAKE', tensor(0), tensor([9.9981e-01, 1.9177e-04]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starting = \"This is a lie. This is a lie. This is a lie. This is a lie. This is a lie.\"\n",
    "learn.predict(starting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "huh. They're all fake. How do I get a real one? Let's grab an actual story from 2016 that wasn't in the original dataset & see how it classifies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('FAKE', tensor(0), tensor([0.9572, 0.0428]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body = \"\"\"President Obama called it, \"an attack of terror and an attack of hate.\" Omar Mateen's early June rampage, in which he gunned down 50 people in the gay nightclub near downtown Orlando, was the deadliest mass shooting in United States history. His comments and perceived allegiance to the Islamic State extremist group were reported as well as reports he had anti-gay views. The FBI said before the 29-year-old committed the massacre, he made allusions to the Islamic State in communications with the agency. Authorities also said Mateen praised allegiance to the Islamic State before he was killed in a hail of gunfire.\"\"\"\n",
    "learn.predict(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that's problematic, because it's literally pulled from USA Today's top stories of 2016, and is absolutely true. It's looking like the model might have overfitted & memorized the dataset. From what the fastAI folks are saying, if `training_loss` << `validation_loss` (which it was during training above), when  you're likely in a case where it's memorized the data. I'm going to leave this in the notebook to show what it looks like when the model overfits.\n",
    "\n",
    "Let's re-do the model, with just one epoch at the final round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g-clef/anaconda3/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.406310</td>\n",
       "      <td>0.295760</td>\n",
       "      <td>0.873717</td>\n",
       "      <td>1.344147</td>\n",
       "      <td>03:37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.339188</td>\n",
       "      <td>0.223076</td>\n",
       "      <td>0.929755</td>\n",
       "      <td>1.249916</td>\n",
       "      <td>04:26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.327676</td>\n",
       "      <td>0.174649</td>\n",
       "      <td>0.930545</td>\n",
       "      <td>1.190829</td>\n",
       "      <td>05:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.165439</td>\n",
       "      <td>0.110357</td>\n",
       "      <td>0.952644</td>\n",
       "      <td>1.116677</td>\n",
       "      <td>07:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = TextDataLoaders.from_csv(path=path, csv_fname=\"news.csv\", text_col=[\"title\", \"text\"], label_col=\"label\", valid_pct=0.2, bs=8)\n",
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.2, metrics=[accuracy, Perplexity()])\n",
    "learn.fit_one_cycle(1)\n",
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1)\n",
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1)\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "95%. with similar `train_loss` and `valid_loss`. Okay. Let's see how it handles the USA Today article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('REAL', tensor(1), tensor([0.4933, 0.5067]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body = \"\"\"President Obama called it, \"an attack of terror and an attack of hate.\" Omar Mateen's early June rampage, in which he gunned down 50 people in the gay nightclub near downtown Orlando, was the deadliest mass shooting in United States history. His comments and perceived allegiance to the Islamic State extremist group were reported as well as reports he had anti-gay views. The FBI said before the 29-year-old committed the massacre, he made allusions to the Islamic State in communications with the agency. Authorities also said Mateen praised allegiance to the Islamic State before he was killed in a hail of gunfire.\"\"\"\n",
    "learn.predict(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real! woot. Not very *confident* about that, but sweet. Let's try adding some blank lines, since the other articles all had blank lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('REAL', tensor(1), tensor([0.0074, 0.9926]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body = \"\"\"President Obama called it, \"an attack of terror and an attack of hate.\" \\n\\nOmar Mateen's early June rampage, in which he gunned down 50 people in the gay nightclub near downtown Orlando, was the deadliest mass shooting in United States history. His comments and perceived allegiance to the Islamic State extremist group were reported as well as reports he had anti-gay views. The FBI said before the 29-year-old committed the massacre, he made allusions to the Islamic State in communications with the agency. \\n\\nAuthorities also said Mateen praised allegiance to the Islamic State before he was killed in a hail of gunfire.\\n\"\"\"\n",
    "learn.predict(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice. Now it's 99% confident it's real. Interesting that just adding a couple newlines brought it up by > 40%. Clearly it learned something about the documents that real ones had blank lines. \n",
    "\n",
    "Out of curiosity, let's try to find how the original data was collected, to see if we can just get another one outside the set to validate.  Apparently the dataset came from this blog post: https://opendatascience.com/how-to-build-a-fake-news-classification-model/ , where he says he got the fake entries from a kaggle competition, and teh real ones from scraping a site called \"allsides.com\". So let's try grabbing an article from allsides & seeing if that comes back as \"REAL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('FAKE', tensor(0), tensor([0.7549, 0.2451]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body = \"\"\"By Daniel Dale, Holmes Lybrand and Tara Subramaniam, CNN\n",
    "\n",
    "Washington (CNN) In an opinion article published on Wednesday, former Vice President Mike Pence did something he used to do in office: echo a lie from former President Donald Trump in a slightly more sophisticated way.\n",
    "Pence's op-ed, published on the Daily Signal website run by the conservative think tank The Heritage Foundation, was mostly filled with attacks on a Democratic elections reform bill known as HR 1.\n",
    "But Pence also made claims about what happened in the 2020 election. Most notably, he began the article by claiming that the election involved \"significant voting irregularities.\"\n",
    "\n",
    "Unlike Trump, Pence did not say the election involved significant \"fraud.\" But he left his vaguer claim about \"voting irregularities\" wide open for readers to interpret as an endorsement of Trump's fraud lie.\n",
    "\n",
    "Facts First: There is no evidence of widespread fraud in the 2020 election. Trump-appointed FBI Director Christopher Wray, who remains in the job under President Joe Biden, testified to Congress on Tuesday that the FBI is \"not aware of any widespread evidence of voter fraud that would have affected the outcome in the election.\" Trump-appointed former Attorney General William Barr said in December, when he was still in the position, that the Justice Department had not seen any such evidence. And no court has endorsed the claim that there was major fraud in the election.\n",
    "\n",
    "Pence's claims about HR 1\n",
    "Now let's turn to Pence's more specific claims about HR 1, which is also known as the For the People Act. The bill would make major changes to numerous elections rules; it also contains significant provisions on government ethics.\n",
    "\n",
    "Voter identification\n",
    "Pence claimed that under the bill, \"voter ID would be banned from coast to coast.\"\n",
    "Facts First: This is false. The bill does not prohibit states from having voter identification requirements. Rather, it requires states to allow voters who do not show ID to instead submit a signed statement under penalty of perjury attesting to their identity and eligibility to vote.\n",
    "Pence could fairly argue that this provision weakens voter ID rules; Heritage Action for America, an advocacy organization connected to The Heritage Foundation, has argued that the provision \"sabotages\" and \"undermines\" state rules. But Pence's declaration that voter ID would be \"banned\" is not true.\n",
    "Undocumented immigrants and voting\n",
    "Pence suggested that the bill would allow undocumented immigrants to register to vote. Specifically, he claimed that by requiring \"automatic voter registration for any individual listed in state and federal government databases,\" like the Department of Motor Vehicles and welfare offices, the bill would ensure \"that millions of illegal immigrants are quickly registered to vote.\"\n",
    "Facts First: This is false. The bill does not change current law that bans people who aren't citizens of the United States, including undocumented immigrants, from voting in federal elections. The bill makes clear that people would still have to affirm that they are US citizens before they are added to the voter rolls. It also says that the government agencies involved in the process are to inform only US citizens that they will be registered to vote unless they choose to opt out. And it says the agencies are required to send state elections officials not only people's names but \"information showing that the individual is a citizen of the United States.\"\n",
    "It is true that in states that already have automatic voter registration, there have sometimes been errors that resulted in non-citizens getting registered to vote. But there are also errors in states without automatic voter registration. Daniel Weiner, deputy director of the Election Reform Program at New York University's liberal Brennan Center for Justice, said in an interview that automatic voter registration \"increases the accuracy\" of the voter rolls, \"not the other way around.\"\n",
    "Regardless: Pence was wrong in suggesting that \"any individual\" listed in government databases would be registered to vote regardless of their citizenship status. \n",
    "\n",
    "\"\"\"\n",
    "learn.predict(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's interesting. This one is also absolutely real, with newlines, but it being flagged as FAKE.\n",
    "\n",
    "Before we dig into that, let's save the model so we can reload it later without having to re-do all the training above (which took a long time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/home/g-clef/turbo_stuff/fake_news_analysis/data/models/unfrozen_training.pth')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save(\"unfrozen_training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last question: what is the model looking at to make the classification determination? Can we introspect the model to see what it's learned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): SentenceEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(36496, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(36496, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): LinBnDrop(\n",
       "        (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Dropout(p=0.08000000000000002, inplace=False)\n",
       "        (2): Linear(in_features=1200, out_features=50, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): LinBnDrop(\n",
       "        (0): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Linear(in_features=50, out_features=2, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
