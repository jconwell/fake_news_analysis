{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "There are two topics to explore: TF-IDF and Stopwords. Be sure to add analysis markdown cells to record any insights you learned, any questions that popped into your head along the way, and any discussion points you want to talk about next time we meet.\n",
    "    \n",
    "Add new cells to do the TFIDF work just below where `df.head(10)` is printed out, above the `Stopwords` section.\n",
    "\n",
    "_Important Note: if you find something interesting in the data that you want to explore, but it isn't part of the homework, immediatly stop what you are doing and EXPLORE IT! Being curious and digging into interesting patterns is more important than completing homework tasks. Just be sure to add markdown cells to record your questions, analysis, findings, and any questions that came up during your analysis_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF: Term Frequency Inverse Document Frequency\n",
    "\n",
    "Here is a good page that describes TFIDF and how to calculate it: http://www.tfidf.com/\n",
    "\n",
    "Calculate the TFIDF scores for the following words: `smell, the, this, washington, money, road, and`\n",
    "\n",
    "How are their scores different from each other? What do you think this means?\n",
    "- how do you interpret words with low scores? What about high scores?\n",
    "\n",
    "What word in the articles has the highest and lowest TFIDF score?\n",
    "\n",
    "More TFIDF resources\n",
    "- https://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html#7990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `news.csv` Data Set\n",
    "\n",
    "4 columns: \n",
    "- article id\n",
    "- article title\n",
    "- article text\n",
    "- lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the dataset: (6335, 4) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6903</td>\n",
       "      <td>Tehran, USA</td>\n",
       "      <td>\\nI’m not an immigrant, but my grandparents ...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7341</td>\n",
       "      <td>Girl Horrified At What She Watches Boyfriend D...</td>\n",
       "      <td>Share This Baylee Luciani (left), Screenshot o...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>95</td>\n",
       "      <td>‘Britain’s Schindler’ Dies at 106</td>\n",
       "      <td>A Czech stockbroker who saved more than 650 Je...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4869</td>\n",
       "      <td>Fact check: Trump and Clinton at the 'commande...</td>\n",
       "      <td>Hillary Clinton and Donald Trump made some ina...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2909</td>\n",
       "      <td>Iran reportedly makes new push for uranium con...</td>\n",
       "      <td>Iranian negotiators reportedly have made a las...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "5        6903                                        Tehran, USA   \n",
       "6        7341  Girl Horrified At What She Watches Boyfriend D...   \n",
       "7          95                  ‘Britain’s Schindler’ Dies at 106   \n",
       "8        4869  Fact check: Trump and Clinton at the 'commande...   \n",
       "9        2909  Iran reportedly makes new push for uranium con...   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  \n",
       "5    \\nI’m not an immigrant, but my grandparents ...  FAKE  \n",
       "6  Share This Baylee Luciani (left), Screenshot o...  FAKE  \n",
       "7  A Czech stockbroker who saved more than 650 Je...  REAL  \n",
       "8  Hillary Clinton and Donald Trump made some ina...  REAL  \n",
       "9  Iranian negotiators reportedly have made a las...  REAL  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the data\n",
    "df=pd.read_csv('data/news.csv')\n",
    "\n",
    "#Get shape and head\n",
    "shape = df.shape\n",
    "print(f\"shape of the dataset: {shape} \\n\")\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_count = dict()\n",
    "counter = 0\n",
    "for body, title in zip(df['text'], df['title']):\n",
    "    all_words = body +  \" \" + title\n",
    "    word_collection = Counter()\n",
    "    for word in re.split(\"\\W+\", all_words):\n",
    "        if word:\n",
    "            word_collection[word.lower()] += 1\n",
    "    word_collection[\"__total__\"] = len(all_words)\n",
    "    term_count[counter] = word_collection\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6335"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_docs = len(term_count.keys())\n",
    "assert num_docs == len(df)\n",
    "num_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.060697912195295"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_idfs = {}\n",
    "all_words = set()\n",
    "for body, title in zip(df[\"text\"], df[\"title\"]):\n",
    "    full_words = body + \" \" + title\n",
    "    full_words = {word.lower() for word in re.split(\"\\W+\", full_words) if word}\n",
    "    all_words.update(full_words)\n",
    "    \n",
    "for word in all_words:\n",
    "    num_docs_with_term = sum(1 for doc_id in term_count if term_count[doc_id][word])\n",
    "    idf = np.log(num_docs/(1+num_docs_with_term))\n",
    "    all_idfs[word] = idf\n",
    "all_idfs[\"all\"]\n",
    "all_idfs[\"s0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(doc_id, term, term_count=term_count, num_docs=num_docs):\n",
    "    tf = term_count[doc_id][term] / term_count[doc_id][\"__total__\"]\n",
    "    idf = all_idfs[term]\n",
    "    result = tf * idf\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term smell\t tf-idf 0.001393225793371872\n",
      "term the\t tf-idf 0.00024359543106162621\n",
      "term this\t tf-idf 0.00017505353807473458\n",
      "term washington\t tf-idf 0.0\n",
      "term money\t tf-idf 0.0\n",
      "term road\t tf-idf 0.0\n",
      "term and\t tf-idf 0.00022259865501157171\n"
     ]
    }
   ],
   "source": [
    "doc_id = 0\n",
    "for term in (\"smell\", \"the\", \"this\", \"washington\", \"money\", \"road\", \"and\"):\n",
    "    t = tf_idf(doc_id, term)\n",
    "    print(f\"term {term}\\t tf-idf {t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as mentioned in other notebook, terms with a small tf-idf are ones that are common both in this document and across all the documents. Large tf-idf means the term is rare in other documents but common (or at least well-represented) in this particular document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's find the min and max tf-idf across all terms in all documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min: (2060, 6.177516654519451e-06, 'to')\n",
      "max: (6328, 0.1905175248085637, 'bum')\n"
     ]
    }
   ],
   "source": [
    "# doc_id, tf-idf, term\n",
    "min_tfidf = (None, 1, None)\n",
    "max_tfidf = (None, 0, None)\n",
    "for doc_id, body, title in zip(range(num_docs), df[\"text\"], df[\"title\"]):\n",
    "    all_words = body + \" \" + title\n",
    "    unique_words = {word.lower() for word in re.split(\"\\W+\", all_words) if word}\n",
    "    scores = {word:tf_idf(doc_id, word) for word in unique_words}\n",
    "    min_term = min(scores, key=scores.get)\n",
    "    max_term = max(scores, key=scores.get)\n",
    "    if scores[min_term] < min_tfidf[1]:\n",
    "        min_tfidf = (doc_id, scores[min_term], min_term)\n",
    "    if scores[max_term] > max_tfidf[1]:\n",
    "        max_tfidf = (doc_id, scores[max_term], max_term)\n",
    "\n",
    "print(f\"min: {min_tfidf}\")\n",
    "print(f\"max: {max_tfidf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"to\" being the minimum doesn't surprise me. It's a common word, so it being super-common in a document and in the whole collection is unsurprising. \"bum\" is a less common word, and used very differently in the UK vs US, so it's not a surprise that it's rarely used in the set of docs, but I'm a little surprised it's used often enough in one doc to give it a big tf-idf. Curious about how many times that word appears in that doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_count[6328][\"bum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(1 for doc_id in term_count if term_count[doc_id][\"bum\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intersting. So the word \"bum\" only appears in 4 documents, appears twice in one of those 4, and therefore has a high idf, and since it's in that doc twice, gets a high tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to's idf: 0.038949242506745134\n",
      "bum's idf: 7.144407180321139\n"
     ]
    }
   ],
   "source": [
    "print(f\"to's idf: {all_idfs['to']}\")\n",
    "print(f\"bum's idf: {all_idfs['bum']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare those to the absolute minimum idf words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min: the 0.019767900195936484 max: stringing 8.060697912195295\n"
     ]
    }
   ],
   "source": [
    "min_idf = min(all_idfs, key=all_idfs.get)\n",
    "max_idf = max(all_idfs, key=all_idfs.get)\n",
    "print(f\"min: {min_idf} {all_idfs[min_idf]} max: {max_idf} {all_idfs[max_idf]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the absolute min and max idfs aren't that far off from the identified min/max tfidf idf values. curious how many docs the max idf is in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(1 for doc_id in term_count if term_count[doc_id][\"stringing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5296]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc_id for doc_id in term_count if term_count[doc_id][\"stringing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_count[5296][\"stringing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0011340317828074417"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf(5296, \"stringing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So \"stringing\" only appears in one document, and so has a quite high idf, but since it only appears once in that document it's overall tf-idf is lower than a word that appears even twice in another document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "\n",
    "TODO: \n",
    "\n",
    "Here are a couple of links about `stopwords` to read. \n",
    "- https://kavita-ganesan.com/what-are-stop-words/\n",
    "- https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html\n",
    "\n",
    "The python NLP toolkit NLTK has a set of built in stopwords it uses for it's algorithms. Install NLTK \n",
    "\n",
    "`pip install nktp`\n",
    "\n",
    "You'll also need to install some of the NLTK resources. Go to this link and follow the cmd line install instructions: https://www.nltk.org/data.html#command-line-installation\n",
    "\n",
    "`python -m nltk.downloader stopwords`\n",
    "\n",
    "Run the below code to print out the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to install the natural language toolkip\n",
    "# $ pip install nltk\n",
    "\n",
    "# to install the \"stopwords\" resource\n",
    "# $ python -m nltk.downloader stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: \n",
    "Given what you learned about TF-IDF, do you think stopwords will have a high TFIDF score or a low score? \n",
    "\n",
    "Why might it be useful to remove stopwords from the text when doing NLP machine learning? What types of words are left over after stopwords are removed?\n",
    "\n",
    "What are some of the potential limitations from removing all the english stopwords when doing news analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd expect stopwords to have a low tf-idf since the point of a stopword is that it's a common, glue-like word, like conjunctions. Two articles can be on immensely different subjects, but I would expect the use of glue words like \"a\", \"an\", \"the\", etc, will be fairly consistent between them, since those are a function of the language generally, rather than the subject. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the above, including stopwords can throw off your statistics about the words in the dataset being analyzed. If you are doing statistics on the most/least common words in the dataset, the presence of words that are simultaneously common and don't convey much meaning will risk throwing off the results of the analysis. \n",
    "\n",
    "For instance, it could be interesting to look into which word is used most often by fake articles compared to real ones (so find minimum idf across the corpus). If you leave stop words in the dataset, that's likely to be a meaningless stop word, and won't give you much useful information to act on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, let's do that. First, the full corpus min idf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min: the 0.019767900195936484\n"
     ]
    }
   ],
   "source": [
    "min_idf = min(all_idfs, key=all_idfs.get)\n",
    "print(f\"min: {min_idf} {all_idfs[min_idf]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to re-compute the `all_idfs` collection for just the reals and fakes. Shouldn't need to re-compute the term_count, since that's per-document, and real vs fake is a document-by-document property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_idfs = dict()\n",
    "real_idfs = dict()\n",
    "all_fake_words = set()\n",
    "all_real_words = set()\n",
    "num_fake_docs = 0\n",
    "num_real_docs = 0\n",
    "# note: tried this with a df.loc to limit it to just fake. that didn't work, so filtering instead.\n",
    "for body, title, label in zip(df[\"text\"], df[\"title\"], df[\"label\"]):\n",
    "    full_words = body + \" \" + title\n",
    "    full_words = {word.lower() for word in re.split(\"\\W+\", full_words) if word}\n",
    "    if label == \"REAL\":\n",
    "        all_real_words.update(full_words)\n",
    "        num_real_docs += 1\n",
    "    elif label == \"FAKE\":\n",
    "        all_fake_words.update(full_words)\n",
    "        num_fake_docs += 1\n",
    "\n",
    "for word in all_fake_words:\n",
    "    num_docs_with_term = sum(1 for doc_id in term_count if term_count[doc_id][word])\n",
    "    idf = np.log(num_fake_docs/(1+num_docs_with_term))\n",
    "    fake_idfs[word] = idf\n",
    "for word in all_real_words:\n",
    "    num_docs_with_term = sum(1 for doc_id in term_count if term_count[doc_id][word])\n",
    "    idf = np.log(num_real_docs/(1+num_docs_with_term))\n",
    "    real_idfs[word] = idf   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake min: the -0.6744848636717585\t real min: the -0.6722749180209556\n"
     ]
    }
   ],
   "source": [
    "min_fake = min(fake_idfs, key=fake_idfs.get)\n",
    "min_real = min(real_idfs, key=real_idfs.get)\n",
    "print(f\"fake min: {min_fake} {fake_idfs[min_fake]}\\t real min: {min_real} {real_idfs[min_real]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "they're the same word for both. Now let's try re-doing this but removing the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_idfs_no_stop = dict()\n",
    "real_idfs_no_stop = dict()\n",
    "all_fake_words = set()\n",
    "all_real_words = set()\n",
    "num_fake_docs = 0\n",
    "num_real_docs = 0\n",
    "set_stops = set(stop)\n",
    "# note: tried this with a df.loc to limit it to just fake. that didn't work, so filtering instead.\n",
    "for body, title, label in zip(df[\"text\"], df[\"title\"], df[\"label\"]):\n",
    "    full_words = body + \" \" + title\n",
    "    full_words = {word.lower() for word in re.split(\"\\W+\", full_words) if word}\n",
    "    full_words = full_words - set_stops\n",
    "    if label == \"REAL\":\n",
    "        all_real_words.update(full_words)\n",
    "        num_real_docs += 1\n",
    "    elif label == \"FAKE\":\n",
    "        all_fake_words.update(full_words)\n",
    "        num_fake_docs += 1\n",
    "\n",
    "for word in all_fake_words:\n",
    "    num_docs_with_term = sum(1 for doc_id in term_count if term_count[doc_id][word])\n",
    "    idf = np.log(num_fake_docs/(1+num_docs_with_term))\n",
    "    fake_idfs_no_stop[word] = idf\n",
    "for word in all_real_words:\n",
    "    num_docs_with_term = sum(1 for doc_id in term_count if term_count[doc_id][word])\n",
    "    idf = np.log(num_real_docs/(1+num_docs_with_term))\n",
    "    real_idfs_no_stop[word] = idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake min: one -0.22793609422421762\t real min: one -0.22572614857341483\n"
     ]
    }
   ],
   "source": [
    "min_fake = min(fake_idfs_no_stop, key=fake_idfs_no_stop.get)\n",
    "min_real = min(real_idfs_no_stop, key=real_idfs_no_stop.get)\n",
    "print(f\"fake min: {min_fake} {fake_idfs_no_stop[min_fake]}\\t real min: {min_real} {real_idfs_no_stop[min_real]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake max: flogs 7.366445148327599\t real max: stringing 7.368655093978402\n"
     ]
    }
   ],
   "source": [
    "max_fake = max(fake_idfs_no_stop, key=fake_idfs_no_stop.get)\n",
    "max_real = max(real_idfs_no_stop, key=real_idfs_no_stop.get)\n",
    "print(f\"fake max: {max_fake} {fake_idfs_no_stop[max_fake]}\\t real max: {max_real} {real_idfs_no_stop[max_real]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so the most common word (min idf) across both real and fake is `one`. Not an enormously surprising word, and not something I would base anything on. The most rare is different between them, and somewhat distinctive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
